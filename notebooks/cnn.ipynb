{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten, MaxPooling2D, Conv2D, Reshape, concatenate,\\\n",
    "Embedding, BatchNormalization, Activation, Dropout, Bidirectional, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from Attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data.columns[2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = data['comment_text'].fillna('UNK')\n",
    "labels = data.loc[:, classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_comments)\n",
    "X_train = pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data[classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153188 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.840B.300d.txt  glove.840B.300d.zip\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error reading word .\n",
      "error reading word at\n",
      "error reading word .\n",
      "error reading word to\n",
      "error reading word .\n",
      "error reading word .\n",
      "error reading word email\n",
      "error reading word or\n",
      "error reading word contact\n",
      "error reading word Email\n",
      "error reading word on\n",
      "error reading word At\n",
      "error reading word by\n",
      "error reading word in\n",
      "error reading word emailing\n",
      "error reading word Contact\n",
      "error reading word at\n",
      "error reading word â€¢\n",
      "error reading word at\n",
      "error reading word is\n",
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../embeddings/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(\"error reading word\", word)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 68661\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153189, 300)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filter_size, sequence_length, embedding_dim):\n",
    "    x = Conv2D(64, (filter_size, embedding_dim))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = MaxPooling2D((sequence_length - filter_size + 1, 1), strides=(1,1))(x)\n",
    "    x = Flatten()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(comment_input)\n",
    "    \n",
    "with tf.device('/gpu:1'):\n",
    "    x = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(x)\n",
    "    conv1 = conv_block(x, 4, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "    conv2 = conv_block(x, 5, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "    conv3 = conv_block(x, 6, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "    conv4 = conv_block(x, 7, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "    x = concatenate([conv1, conv2, conv3, conv4])\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    predictions = Dense(6, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(comment_input, predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 150, 300)     45956700    input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 150, 300, 1)  0           embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 147, 1, 64)   76864       reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 146, 1, 64)   96064       reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 145, 1, 64)   115264      reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 144, 1, 64)   134464      reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 147, 1, 64)   256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 146, 1, 64)   256         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 145, 1, 64)   256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 144, 1, 64)   256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 147, 1, 64)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 146, 1, 64)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 145, 1, 64)   0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 144, 1, 64)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 147, 1, 64)   0           activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 146, 1, 64)   0           activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 145, 1, 64)   0           activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 144, 1, 64)   0           activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling2D) (None, 1, 1, 64)     0           dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_55 (MaxPooling2D) (None, 1, 1, 64)     0           dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_56 (MaxPooling2D) (None, 1, 1, 64)     0           dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_57 (MaxPooling2D) (None, 1, 1, 64)     0           dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_54 (Flatten)            (None, 64)           0           max_pooling2d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_55 (Flatten)            (None, 64)           0           max_pooling2d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_56 (Flatten)            (None, 64)           0           max_pooling2d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_57 (Flatten)            (None, 64)           0           max_pooling2d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 256)          0           flatten_54[0][0]                 \n",
      "                                                                 flatten_55[0][0]                 \n",
      "                                                                 flatten_56[0][0]                 \n",
      "                                                                 flatten_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 256)          65792       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 256)          1024        dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 256)          0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 256)          0           activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 64)           16448       dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 64)           256         dense_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 64)           0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 64)           0           activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 6)            390         dropout_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 46,464,290\n",
      "Trainable params: 506,438\n",
      "Non-trainable params: 45,957,852\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_ckpt = ModelCheckpoint(filepath='../models/cnn.h5', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3)\n",
    "callbacks = [early_stopping, model_ckpt, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/10\n",
      "86265/86265 [==============================] - 49s 573us/step - loss: 0.1948 - acc: 0.9435 - val_loss: 0.1368 - val_acc: 0.9630\n",
      "Epoch 2/10\n",
      "86265/86265 [==============================] - 41s 476us/step - loss: 0.0488 - acc: 0.9822 - val_loss: 0.0698 - val_acc: 0.9765\n",
      "Epoch 3/10\n",
      "86265/86265 [==============================] - 41s 474us/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0465 - val_acc: 0.9822\n",
      "Epoch 4/10\n",
      "86265/86265 [==============================] - 40s 467us/step - loss: 0.0357 - acc: 0.9859 - val_loss: 0.0828 - val_acc: 0.9782\n",
      "Epoch 5/10\n",
      "86265/86265 [==============================] - 40s 467us/step - loss: 0.0317 - acc: 0.9873 - val_loss: 0.0495 - val_acc: 0.9819\n",
      "Epoch 6/10\n",
      "86265/86265 [==============================] - 40s 467us/step - loss: 0.0284 - acc: 0.9885 - val_loss: 0.0492 - val_acc: 0.9817\n",
      "Epoch 7/10\n",
      "86265/86265 [==============================] - 42s 484us/step - loss: 0.0261 - acc: 0.9893 - val_loss: 0.0524 - val_acc: 0.9804\n",
      "Epoch 8/10\n",
      "86265/86265 [==============================] - 40s 466us/step - loss: 0.0208 - acc: 0.9918 - val_loss: 0.0612 - val_acc: 0.9824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0759a85a58>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=512, epochs=10, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = test['comment_text'].fillna('UNK')\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "X_test = pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:, 1:] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../submissions/cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0   6044863  0.001792      0.000018  0.000174  0.000042  0.000131   \n",
       "1   6102620  0.002217      0.000024  0.000651  0.000045  0.000334   \n",
       "2  14563293  0.000448      0.000037  0.000355  0.000284  0.000304   \n",
       "3  21086297  0.003724      0.000039  0.000699  0.000119  0.000415   \n",
       "4  22982444  0.000980      0.000008  0.000123  0.000020  0.000093   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.000048  \n",
       "1       0.000216  \n",
       "2       0.000301  \n",
       "3       0.000135  \n",
       "4       0.000020  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../submissions/cnn.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
