{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation, BatchNormalization, AveragePooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from Attention import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(len(data)).reshape(-1,1)\n",
    "ros = RandomOverSampler()\n",
    "balanced_index, balanced_labels = ros.fit_sample(index, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_index = balanced_index.flatten()\n",
    "balanced_labels = balanced_labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[balanced_index, :]\n",
    "y_train = to_categorical(balanced_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    19190\n",
       "1    19190\n",
       "0    19190\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data.tweet\n",
    "labels = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(tweets)\n",
    "X_train = np.array([np.array(_) for _ in train_sequences])\n",
    "X_train = pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36508 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error reading word .\n",
      "error reading word at\n",
      "error reading word .\n",
      "error reading word to\n",
      "error reading word .\n",
      "error reading word .\n",
      "error reading word email\n",
      "error reading word or\n",
      "error reading word contact\n",
      "error reading word Email\n",
      "error reading word on\n",
      "error reading word At\n",
      "error reading word by\n",
      "error reading word in\n",
      "error reading word emailing\n",
      "error reading word Contact\n",
      "error reading word at\n",
      "error reading word â€¢\n",
      "error reading word at\n",
      "error reading word is\n",
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../embeddings/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(\"error reading word\", word)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 17622\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36509, 300)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = Input(shape=[MAX_SEQUENCE_LENGTH], dtype='int32')\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    x = embedding_layer(tweet_input)\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    x = Bidirectional(LSTM(300, dropout=0.1, return_sequences=True, recurrent_dropout=0.1))(x)\n",
    "    x = Attention()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    predictions = Dense(3, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(tweet_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 300)          10952700  \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 100, 600)          1442400   \n",
      "_________________________________________________________________\n",
      "attention_4 (Attention)      (None, 600)               700       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               60100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 12,456,603\n",
      "Trainable params: 1,503,703\n",
      "Non-trainable params: 10,952,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_ckpt = ModelCheckpoint(filepath='../models/sentiment_analysis.h5', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
    "callbacks = [early_stopping, model_ckpt, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_generator(X, y, batch_size):\n",
    "#     idx = np.arange(X)\n",
    "#     while True:\n",
    "#         np.random.shuffle(idx)\n",
    "#         X = X[idx]\n",
    "#         y = y[idx]\n",
    "#         for i in range(0, len(X)):\n",
    "#             yield (pad_sequences(X[i:i + batch_size]), y[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46056 samples, validate on 11514 samples\n",
      "Epoch 1/20\n",
      "46056/46056 [==============================] - 70s 2ms/step - loss: 0.7216 - acc: 0.7058 - val_loss: 1.5629 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "46056/46056 [==============================] - 68s 1ms/step - loss: 0.4663 - acc: 0.8295 - val_loss: 1.4403 - val_acc: 0.0323\n",
      "Epoch 3/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.3466 - acc: 0.8718 - val_loss: 1.3437 - val_acc: 0.3397\n",
      "Epoch 4/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.2628 - acc: 0.9049 - val_loss: 0.1205 - val_acc: 0.9662\n",
      "Epoch 5/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.1955 - acc: 0.9316 - val_loss: 0.4252 - val_acc: 0.8372\n",
      "Epoch 6/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.1464 - acc: 0.9485 - val_loss: 0.1111 - val_acc: 0.9667\n",
      "Epoch 7/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.1144 - acc: 0.9608 - val_loss: 0.1320 - val_acc: 0.9448\n",
      "Epoch 8/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0876 - acc: 0.9699 - val_loss: 0.1465 - val_acc: 0.9468\n",
      "Epoch 9/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0663 - acc: 0.9772 - val_loss: 0.0321 - val_acc: 0.9917\n",
      "Epoch 10/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0532 - acc: 0.9813 - val_loss: 0.0668 - val_acc: 0.9812\n",
      "Epoch 11/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0448 - acc: 0.9849 - val_loss: 0.0056 - val_acc: 0.9989\n",
      "Epoch 12/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0371 - acc: 0.9874 - val_loss: 0.0247 - val_acc: 0.9933\n",
      "Epoch 13/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0349 - acc: 0.9886 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Epoch 14/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0291 - acc: 0.9903 - val_loss: 0.0999 - val_acc: 0.9613\n",
      "Epoch 15/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0219 - acc: 0.9922 - val_loss: 0.0113 - val_acc: 0.9961\n",
      "Epoch 16/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0229 - acc: 0.9923 - val_loss: 0.0074 - val_acc: 0.9970\n",
      "Epoch 17/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0082 - val_acc: 0.9982\n",
      "Epoch 18/20\n",
      "46056/46056 [==============================] - 67s 1ms/step - loss: 0.0212 - acc: 0.9936 - val_loss: 0.0043 - val_acc: 0.9984\n",
      "Epoch 19/20\n",
      "46056/46056 [==============================] - 68s 1ms/step - loss: 0.0177 - acc: 0.9944 - val_loss: 0.0035 - val_acc: 0.9979\n",
      "Epoch 20/20\n",
      "46056/46056 [==============================] - 68s 1ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0023 - val_acc: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2560a2c18>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=256, epochs=20, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = tokenizer.texts_to_sequences(train['comment_text'])\n",
    "train_comments = pad_sequences(train_comments, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs = model.predict(train_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prob_hate_speech'] = np.nan\n",
    "train['prob_offensive_language'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>prob_hate_speech</th>\n",
       "      <th>prob_offensive_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  prob_hate_speech  \\\n",
       "0             0        0       0       0              0               NaN   \n",
       "1             0        0       0       0              0               NaN   \n",
       "2             0        0       0       0              0               NaN   \n",
       "3             0        0       0       0              0               NaN   \n",
       "4             0        0       0       0              0               NaN   \n",
       "\n",
       "   prob_offensive_language  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, ['prob_hate_speech', 'prob_offensive_language']] = train_probs[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/processed/train_sent_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = tokenizer.texts_to_sequences(test['comment_text'].fillna('UNK'))\n",
    "test_comments = pad_sequences(test_comments, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['prob_hate_speech'] = np.nan\n",
    "test['prob_offensive_language'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, ['prob_hate_speech', 'prob_offensive_language']] = test_probs[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>prob_hate_speech</th>\n",
       "      <th>prob_offensive_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "      <td>0.042976</td>\n",
       "      <td>1.362989e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>6.388481e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>8.061553e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "      <td>0.024494</td>\n",
       "      <td>3.593352e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "      <td>0.025673</td>\n",
       "      <td>5.709131e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  \\\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...   \n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...   \n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...   \n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...   \n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] ==   \n",
       "\n",
       "   prob_hate_speech  prob_offensive_language  \n",
       "0          0.042976             1.362989e-01  \n",
       "1          0.000807             6.388481e-03  \n",
       "2          0.000168             8.061553e-07  \n",
       "3          0.024494             3.593352e-02  \n",
       "4          0.025673             5.709131e-02  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
