{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk1/home/sum5275/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/disk1/home/sum5275/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten, MaxPooling2D, Conv2D, Reshape, concatenate,\\\n",
    "Embedding, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['toxic', 'severe_toxic',\n",
    "           'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = data['comment_text'].fillna('UNK').values\n",
    "y = data[classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments, val_comments, y_train, y_val = train_test_split(comments, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_comments)\n",
    "val_sequences   = tokenizer.texts_to_sequences(val_comments)\n",
    "X_train         = pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "X_val           = pad_sequences(val_sequences, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 183447 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.840B.300d.txt  glove.840B.300d.zip\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error reading word .\n",
      "error reading word at\n",
      "error reading word .\n",
      "error reading word to\n",
      "error reading word .\n",
      "error reading word .\n",
      "error reading word email\n",
      "error reading word or\n",
      "error reading word contact\n",
      "error reading word Email\n",
      "error reading word on\n",
      "error reading word At\n",
      "error reading word by\n",
      "error reading word in\n",
      "error reading word emailing\n",
      "error reading word Contact\n",
      "error reading word at\n",
      "error reading word â€¢\n",
      "error reading word at\n",
      "error reading word is\n",
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../embeddings/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(\"error reading word\", word)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 88153\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183448, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filter_size, sequence_length, embedding_dim):\n",
    "    x = Conv2D(64, (filter_size, embedding_dim))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = MaxPooling2D((sequence_length - filter_size + 1, 1), strides=(1,1))(x)\n",
    "    x = Flatten()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(comment_input)\n",
    "\n",
    "x = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(x)\n",
    "conv1 = conv_block(x, 4, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "conv2 = conv_block(x, 5, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "conv3 = conv_block(x, 6, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "x = concatenate([conv1, conv2, conv3])\n",
    "\n",
    "x = Dense(100)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# x = Dense(64)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "\n",
    "predictions = Dense(6, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(comment_input, predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     55034400    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 150, 300, 1)  0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 147, 1, 64)   76864       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 146, 1, 64)   96064       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 145, 1, 64)   115264      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 147, 1, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 146, 1, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 145, 1, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 147, 1, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 146, 1, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 145, 1, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 147, 1, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 146, 1, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 145, 1, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 64)     0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 64)     0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 64)     0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 64)           0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64)           0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          19300       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            606         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 55,343,666\n",
      "Trainable params: 308,682\n",
      "Non-trainable params: 55,034,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_ckpt = ModelCheckpoint(filepath='../models/cnn.h5', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3)\n",
    "callbacks = [model_ckpt, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/5\n",
      "127656/127656 [==============================] - 45s 356us/step - loss: 0.0875 - acc: 0.9708 - val_loss: 0.0499 - val_acc: 0.9815\n",
      "Epoch 2/5\n",
      "127656/127656 [==============================] - 42s 333us/step - loss: 0.0443 - acc: 0.9830 - val_loss: 0.0552 - val_acc: 0.9806\n",
      "Epoch 3/5\n",
      "127656/127656 [==============================] - 43s 338us/step - loss: 0.0397 - acc: 0.9842 - val_loss: 0.0481 - val_acc: 0.9824\n",
      "Epoch 4/5\n",
      "127656/127656 [==============================] - 43s 338us/step - loss: 0.0358 - acc: 0.9855 - val_loss: 0.0442 - val_acc: 0.9830\n",
      "Epoch 5/5\n",
      "127656/127656 [==============================] - 42s 333us/step - loss: 0.0326 - acc: 0.9867 - val_loss: 0.0491 - val_acc: 0.9824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6cfdc94cf8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=256,\n",
    "          epochs=5, callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31915/31915 [==============================] - 6s 183us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = model.predict(X_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31915, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic           log_loss: 0.10 auc: 0.98 acc: 0.96\n",
      "\n",
      "[[28656   215]\n",
      " [ 1000  2044]]\n",
      "\n",
      "severe_toxic    log_loss: 0.03 auc: 0.99 acc: 0.99\n",
      "\n",
      "[[31544    49]\n",
      " [  246    76]]\n",
      "\n",
      "obscene         log_loss: 0.05 auc: 0.99 acc: 0.98\n",
      "\n",
      "[[30121   126]\n",
      " [  490  1178]]\n",
      "\n",
      "threat          log_loss: 0.01 auc: 0.98 acc: 1.00\n",
      "\n",
      "[[31818    16]\n",
      " [   56    25]]\n",
      "\n",
      "insult          log_loss: 0.08 auc: 0.98 acc: 0.97\n",
      "\n",
      "[[30135   170]\n",
      " [  765   845]]\n",
      "\n",
      "identity_hate   log_loss: 0.03 auc: 0.98 acc: 0.99\n",
      "\n",
      "[[31613    29]\n",
      " [  212    61]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.DataFrame()\n",
    "for i, col in enumerate(classes):\n",
    "    loss = log_loss(y_val[:, i], y_pred_val[:, i])\n",
    "    auc = roc_auc_score(y_val[:, i], y_pred_val[:, i])\n",
    "    acc = accuracy_score(y_val[:, i], (y_pred_val[:, i] > .5).astype(int))\n",
    "    val_df = val_df.append({'class': col, 'auc': auc, 'loss': loss, 'acc': acc}, ignore_index=True)\n",
    "    print('{:15} log_loss: {:.2f} auc: {:.2f} acc: {:.2f}'.format(col, loss, auc, acc))\n",
    "    print()\n",
    "    print(confusion_matrix(y_val[:, i], (y_pred_val[:, i] > .5).astype(int)))\n",
    "    print()\n",
    "val_df = val_df.set_index('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc     0.982380\n",
       "auc     0.983347\n",
       "loss    0.049142\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = test['comment_text'].fillna('UNK')\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "X_test = pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 27s 179us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:, 1:] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('../submissions/cnn_300.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.899014</td>\n",
       "      <td>6.560973e-02</td>\n",
       "      <td>0.636133</td>\n",
       "      <td>1.711239e-02</td>\n",
       "      <td>0.498962</td>\n",
       "      <td>4.544874e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.005194</td>\n",
       "      <td>8.586932e-08</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>9.830270e-09</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>4.422942e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>6.701796e-09</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>3.856752e-10</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>3.213974e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>6.218900e-11</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>1.383176e-08</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.032655e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>6.080754e-07</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>1.510305e-06</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>8.547210e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
       "0  00001cee341fdb12  0.899014  6.560973e-02  0.636133  1.711239e-02  0.498962   \n",
       "1  0000247867823ef7  0.005194  8.586932e-08  0.000779  9.830270e-09  0.000243   \n",
       "2  00013b17ad220c46  0.000398  6.701796e-09  0.000135  3.856752e-10  0.000026   \n",
       "3  00017563c3f7919a  0.000285  6.218900e-11  0.000017  1.383176e-08  0.000014   \n",
       "4  00017695ad8997eb  0.004565  6.080754e-07  0.000887  1.510305e-06  0.000159   \n",
       "\n",
       "   identity_hate  \n",
       "0   4.544874e-02  \n",
       "1   4.422942e-06  \n",
       "2   3.213974e-07  \n",
       "3   1.032655e-08  \n",
       "4   8.547210e-06  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../submissions/cnn_300.csv').head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
